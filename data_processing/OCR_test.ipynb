{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " path ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "\n",
      "ROOT_DIR : Z:\\maeng_dir\\sparta_camp\\final_sparta\n",
      "pwd : z:\\maeng_dir\\sparta_camp\\final_sparta\\ipython\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "#### path\n",
    "print(\"\\n\\n path ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\\n\")\n",
    "load_dotenv()\n",
    "ROOT_DIR = os.getenv(\"ROOT_DIR\")\n",
    "poppler_path = os.path.join(ROOT_DIR, r\"lib\\poppler-24.08.0\\Library\\bin\")\n",
    "\n",
    "\n",
    "print(\"ROOT_DIR :\", ROOT_DIR)\n",
    "print(\"pwd :\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR - pdf to text\n",
    "https://sooeun67.github.io/data%20science/ocr-comparison/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "#### Data Load & Split\n",
    "print(\"\\n\\n Data Load & Split ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\\n\")\n",
    "whole_data = CSVLoader(file_path=\"./data/test.csv\", encoding='utf-8').load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)\n",
    "split_data = splitter.split_documents(whole_data)\n",
    "\n",
    "\n",
    "print(type(split_data[0]))\n",
    "print(split_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders.parsers import RapidOCRBlobParser\n",
    "\n",
    "\n",
    "#### path\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "pdf_image_path_05 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_05.pdf\")\n",
    "pdf_image_path = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"soil_suitability.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Data Load & Split\n",
    "print(\"\\n\\n Data Load & Split ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\\n\")\n",
    "# page_list = PyPDFLoader(file_path = pdf_image_path_01,).load()\n",
    "# page_list = PyPDFLoader(file_path = pdf_image_path_01, mode=\"page\").load()\n",
    "# page_list = PyPDFLoader(file_path = pdf_image_path_01, extract_images=True).load()\n",
    "\n",
    "page_list = PyPDFLoader(\n",
    "            file_path = pdf_image_path_01,\n",
    "            mode=\"page\",\n",
    "            images_inner_format=\"markdown-img\",\n",
    "            images_parser=RapidOCRBlobParser(),\n",
    "            ).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page_list[0])\n",
    "# print(page_list[0])\n",
    "# print(page_list[1])\n",
    "# print(page_list[1].page_content[:300])\n",
    "# print(page_list[2])\n",
    "# print(page_list[2].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pytesseract\n",
    "\n",
    "\n",
    "#### path\n",
    "# pytesseract.pytesseract.tesseract_cmd = r\"C:\\Users\\USER\\AppData\\Local\\Tesseract-OCR\\tesseract.exe\"\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "pdf_image_path_05 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_05.pdf\")\n",
    "pdf_image_path = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"soil_suitability.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " page 0 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "AA\n",
      "\n",
      "melonggena 7.\n",
      "\n",
      "생육에 따른 기상득성(0)\n",
      "\n",
      "sores                   a\n",
      "\n",
      "직물 재배에 FLAP 화혁싱\n",
      "\n",
      "re |   소젠        —\n",
      "ais | BES] 거소    !      Me | PS\n",
      "ase                     Erle                      ure\n",
      "Tian Dai Pan\n",
      "\n",
      "\n",
      " page 1 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "생육에 따른 기상두성(0)\n",
      "Ed          1    [    STG\n",
      "\n",
      "To 515. 1\n",
      "\n",
      "Amel 알맞은 도양 ada 득성 및 Sela)\n",
      "\n",
      "1        ae        두실 | ae Foo\n",
      "z       100\n",
      "\n",
      "직물 Add 알맛은 Ve Bad\n",
      "\n",
      "ml         Feccations\n",
      "\n",
      "ow |        1s Shel Becirs wt\n",
      "\n",
      "a                    My\n",
      "\n",
      "ele         더                   마의\n",
      "\n",
      "ma | ena          Wea | 1815\n",
      "\n",
      "=~\n",
      "\n",
      " page 2 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "생육에 따는 기상두\n",
      "\n",
      "Toe\n",
      "\n",
      "Zee\n",
      "\n",
      "Ae 제배에 알맞은 노\n",
      "\n",
      "기명    경사!\n",
      "\n",
      "두신\n",
      "\n",
      "당일 ae\n",
      "\n",
      "ee eo\n",
      "\n",
      "재매에 알맞은 도양 sa) 4)\n",
      "re            nella            Feccations\n",
      "은                           clea\n",
      "ase | OS                      Me\n",
      "lint                        wa\n",
      "= am                  0907\n",
      "\n",
      "Jatt FARR 가기\n",
      "\n",
      "asl\n",
      "\n",
      "goo}, ola)\n",
      "\n",
      " page 3 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "=\n",
      "\n",
      "생육에 따른 기상두성(\n",
      "\n",
      "a\n",
      "\n",
      "TEE\n",
      "\n",
      "Saale       a\n",
      "\n",
      "가을 제때에 TSP ey 형데식 득싱 및 을리성\n",
      "7        ae           za       fata\n",
      "Ee       ;     aaa\n",
      "\n",
      "가을 제매에 RP 노양 회학성\n",
      "\n",
      "-         ee,\n",
      "ve          ar      VRE OLS\n",
      "\n",
      "사도\n",
      "\n",
      "-28-\n",
      "\n",
      " page 4 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "5) ase\n",
      "\n",
      "us midanvis nar f\n",
      "\n",
      "생육에 따른 기상득성(0)\n",
      "\n",
      "Dee\n",
      "\n",
      "Taye\n",
      "\n",
      "ae 제배에 안낮은 노암 혐데석 두성\n",
      "\n",
      "|        4\n",
      "\n",
      "mal\n",
      "\n",
      "Ae 제배에 PEP ET 회학성\n",
      "\n",
      "eR,\n",
      "\n",
      "매 | ow | UE\n",
      "\n",
      "4\n",
      "\n",
      "ala\n",
      "\n",
      "a\n",
      "\n",
      "\n",
      " page 5 ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\n",
      "=A\n",
      "\n",
      "재매에 알맞은 도양 Sha] 4)\n",
      "1,         Feccations\n",
      "와\n",
      "\n",
      "미 | ow\n",
      "Bs\n",
      "\n",
      "사연\n",
      "Ca         Mi\n",
      "ali\n",
      "\n",
      "0020\n",
      "\n",
      "ㅠㅠ\n",
      "\n",
      "-3-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### OCR\n",
    "with pdfplumber.open(pdf_image_path) as pdf:\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        page_image = page.to_image()\n",
    "        full_page_image = page_image.original.convert(\"RGB\") # PIL 이미지로 변환\n",
    "\n",
    "        ocr_text = pytesseract.image_to_string(full_page_image, lang=\"kor+eng\")\n",
    "        \n",
    "        print(f\" page {i} ㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡㅡ\")\n",
    "        print(ocr_text)\n",
    "        \n",
    "        if i == 5:\n",
    "            break\n",
    "        \n",
    "        # page_list[i] = Document(page_content=ocr_text, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pypdfium2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pypdfium2\n",
    "import pypdfium2 as pdfium\n",
    "import os\n",
    "\n",
    "\n",
    "#### path\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "pdf_image_path_05 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_05.pdf\")\n",
    "pdf_image_path = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"soil_suitability.pdf\")\n",
    "\n",
    "\n",
    "pdf = pdfium.PdfDocument(pdf_image_path_01)\n",
    "\n",
    "version = pdf.get_version()  # get the PDF standard version\n",
    "n_pages = len(pdf)  # get the number of pages in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdf)):\n",
    "    page = pdf[i]  \n",
    "    text = page.get_textpage().get_text_range()\n",
    "    # text = page.get_textpage().get_text_bounded()\n",
    "    \n",
    "    print(\"text :\", text)\n",
    "    # break\n",
    "    # extracted_text.append(text)\n",
    "\n",
    "# \"\\n\".join(extracted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ghostscript - pdf to pdf (with text)\n",
    "https://ghostscript.com/blog/ocr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "#### path\n",
    "gs_path = r\"C:\\Program Files\\gs\\gs10.04.0\\bin\\gswin64c.exe\"\n",
    "os.environ[\"TESSDATA_PREFIX\"] = r\"C:\\Program Files\\Tesseract-OCR\\tessdata\"\n",
    "\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "\n",
    "output_pdf_path = os.path.join(ROOT_DIR, \"data\", \"output.PDF\")\n",
    "output_txt_path = os.path.join(ROOT_DIR, \"data\", \"ocr_text.txt\")\n",
    "output_json_path = os.path.join(ROOT_DIR, \"data\", \"ocr_text.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ocr\n",
    "def ocr_with_ghostscript(input_pdf, output_pdf, lang=\"kor\"):\n",
    "    gs_command = [\n",
    "        gs_path,\n",
    "        # \"gs\",\n",
    "        \"-dNOPAUSE\",\n",
    "        \"-dBATCH\",\n",
    "        \"-sDEVICE=ocr\",\n",
    "        f\"-sOCRLanguage={lang}\",  # 한글 OCR 설정\n",
    "        \"-r300\",\n",
    "        f\"-sOutputFile={output_pdf}\",\n",
    "        input_pdf\n",
    "    ]\n",
    "    \n",
    "    result = subprocess.run(gs_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"OCR 적용 완료: {output_pdf}\")\n",
    "    else:\n",
    "        print(f\"오류 발생: {result.stderr.decode()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_with_ghostscript(pdf_image_path_01, output_pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aspose\n",
    "https://docs.aspose.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'aspose.ocr.RecognitionSettings' object has no attribute 'auto_denoising'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 인식 설정 초기화\u001b[39;00m\n\u001b[0;32m      8\u001b[0m settings \u001b[38;5;241m=\u001b[39m ocr\u001b[38;5;241m.\u001b[39mRecognitionSettings()\n\u001b[1;32m----> 9\u001b[0m \u001b[43msettings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauto_denoising\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m settings\u001b[38;5;241m.\u001b[39mauto_contrast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# 인식 배치에 파일 추가\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'aspose.ocr.RecognitionSettings' object has no attribute 'auto_denoising'"
     ]
    }
   ],
   "source": [
    "# 이 코드 예제는 Python에서 스캔한 PDF 문서에서 텍스트를 인식하고 추출하는 방법을 보여줍니다.\n",
    "import aspose.ocr as ocr\n",
    "import os\n",
    "\n",
    "\n",
    "#### path\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "\n",
    "\n",
    "# OCR 엔진 초기화\n",
    "api = ocr.AsposeOcr()\n",
    "\n",
    "# 인식 설정 초기화\n",
    "settings = ocr.RecognitionSettings()\n",
    "settings.auto_denoising = True\n",
    "settings.auto_contrast = True\n",
    "\n",
    "# 인식 배치에 파일 추가\n",
    "files = ocr.OcrInput(ocr.InputType.PDF)\n",
    "\n",
    "# 스캔한 PDF에 액세스하여 페이지 번호와 총 페이지 수를 설정하세요.\n",
    "files.add(pdf_image_path_01, 0, 1)\n",
    "\n",
    "# 텍스트를 인식\n",
    "result = api.recognize(files , settings)\n",
    "\n",
    "# 인식 결과 인쇄\n",
    "print(result[0].recognition_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pdf to image pdf2imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pdf2image\n",
    "import pdf2image\n",
    "from pdf2image import convert_from_path\n",
    "import os\n",
    "\n",
    "\n",
    "#### path\n",
    "poppler_path = os.path.join(ROOT_DIR, r\"lib\\poppler-24.08.0\\Library\\bin\")\n",
    "\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### convert\n",
    "pages = convert_from_path(pdf_image_path, poppler_path=poppler_path)\n",
    "\n",
    "for count, page in enumerate(pages):\n",
    "    print(count)\n",
    "    # break\n",
    "    # page.save(f'out{count}.jpg', 'JPEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytesseract (with image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "#### lib path\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "poppler_path = os.path.join(ROOT_DIR, r\"lib\\poppler-24.08.0\\Library\\bin\")\n",
    "\n",
    "pdf_text_path = os.path.join(ROOT_DIR, \"data\", \"sample_text.pdf\")\n",
    "pdf_image_path_01 = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### convert\n",
    "images = convert_from_path(\n",
    "                            pdf_image_path_01,\n",
    "                            dpi = 300,\n",
    "                            poppler_path=poppler_path,\n",
    "                        )\n",
    "\n",
    "f = open(\"./ocr_text.txt\", \"w\")\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    ocr_text = pytesseract.image_to_string(image, lang=\"kor+eng\")\n",
    "    f.write(f\"page {i}\")\n",
    "    f.write(ocr_text)\n",
    "    \n",
    "f.close()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naver clova\n",
    "pdf to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\000_009.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\010_019.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\020_029.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\030_039.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\040_049.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\050_059.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\060_069.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\070_079.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\080_089.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\090_099.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\100_109.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\110_119.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\120_129.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\130_139.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\140_149.pdf',\n",
       " 'Z:\\\\maeng_dir\\\\sparta_camp\\\\final_sparta\\\\data\\\\soil_suitability\\\\150_159.pdf']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import uuid\n",
    "import time\n",
    "import json\n",
    "\n",
    "\n",
    "#### load keys\n",
    "load_dotenv()\n",
    "api_url = os.getenv(\"invoke_url\")\n",
    "secret_key = os.getenv(\"secret_key\")\n",
    "\n",
    "\n",
    "#### path\n",
    "# pdf_path = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_01.pdf\")\n",
    "# pdf_path = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"sample_image_05.pdf\")\n",
    "# pdf_path = os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", \"soil_suitability_160.pdf\")\n",
    "\n",
    "pdf_path_list = [os.path.join(ROOT_DIR, \"data\", \"soil_suitability\", f\"{str(num).zfill(2)}0_{str(num).zfill(2)}9.pdf\")for num in range (0, 16)]\n",
    "\n",
    "\n",
    "\n",
    "pdf_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### api params\n",
    "pdf_path = pdf_path_list[1]\n",
    "\n",
    "request_json = {\n",
    "    'images': [\n",
    "        {\n",
    "            'format': 'pdf',\n",
    "            'name': 'demo'\n",
    "        }\n",
    "    ],\n",
    "    'requestId': str(uuid.uuid4()),\n",
    "    'version': 'V2',\n",
    "    'timestamp': int(round(time.time() * 1000))\n",
    "}\n",
    "\n",
    "payload = {'message': json.dumps(request_json).encode('UTF-8')}\n",
    "files = [\n",
    "    ('file', open(pdf_path,'rb'))\n",
    "]\n",
    "headers = {\n",
    "  'X-OCR-SECRET': secret_key\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#### api\n",
    "response = requests.request(\"POST\", api_url, headers=headers, data = payload, files = files)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### for checking data (to text)\n",
    "pages = response.json()['images']\n",
    "\n",
    "fw = open(\"../test_ocr_naver.txt\", \"w\")\n",
    "for page in pages:\n",
    "    for i_obj, obj in enumerate(page['fields']):\n",
    "        text = obj['inferText']\n",
    "        print(obj, file = fw)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#### save raw result\n",
    "print(type(response.json()))\n",
    "\n",
    "with open(\"../raw_reponse_naver_ocr.json\", \"w\") as f:\n",
    "    f.write(json.dumps(response.json(), ensure_ascii=False, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_sparta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
